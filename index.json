[{"authors":["admin"],"categories":null,"content":"I am a researcher and engineer in Artificial Intelligence with mixed academia and industry experiences for about 10 years. My background covers the fields of deep learning and computer vision. Currently, I am a technical leader in kujiale.com, which is a unicorn startup company in the field of clould computing, 3D interior design and visulization. Besides exploring the edge of artificial intelligence, I also have some interests in social sciences and humanities because of the curiosity and care of our world. My hobbies are sports and travel. If you have any question about me and my work, don\u0026rsquo;t hesitate to make a contact.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://by-liu.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a researcher and engineer in Artificial Intelligence with mixed academia and industry experiences for about 10 years. My background covers the fields of deep learning and computer vision. Currently, I am a technical leader in kujiale.com, which is a unicorn startup company in the field of clould computing, 3D interior design and visulization. Besides exploring the edge of artificial intelligence, I also have some interests in social sciences and humanities because of the curiosity and care of our world.","tags":null,"title":"Bingyuan Liu","type":"authors"},{"authors":[],"categories":[],"content":"I just spent two months\u0026rsquo; wonderful experience as a volunteer section leader in Standford Code in Place, which is a community-service virtual coding education program in the time of COVID-19. It is a very special and history making course with over 800 great teachers and 8000 students all over the world.\nCode in Place is an introductory programming course using the Python language, with material from the first half of Stanford University\u0026rsquo;s established intro course, CS106A. It is operated by some great Stanford professors and instructors. As far as I know, this is the best online course nowadays as it creates a learning community with some great ideas and tools like section, Ed and Zoom.\nIn this journey, I lead a section with 8 lovely students. Section is what makes CodeInPlace different from other online courses. It is a small learning group and sectionees meet via videoconferencing once a week with a section leader. In every section meeting, I helped consolidate students\u0026rsquo; understanding of the concepts presented in lectures and gave students the opportunity to problem-solve in an active learning setting by having them come up with the solutions with my facilitation. This is my first teaching experience and it is such a great start with so many tips and inspirations from the best instructors. I feel very enjoyable and grateful to be part of this community. Thanks for all the instructors and my lovely students. In this COVID time, it\u0026rsquo;s so special and I will definitely miss it.\n","date":1590239640,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590239640,"objectID":"e6ab56a3fa78715f2a8be2e27e44b345","permalink":"https://by-liu.github.io/post/section-leader-in-cs106a/","publishdate":"2020-05-23T21:14:00+08:00","relpermalink":"/post/section-leader-in-cs106a/","section":"post","summary":"I served as a section leader in Standford Code in Place from Apr 2020 to May 2020, which is a online coding education program.","tags":["teach"],"title":"A wonderful experience as a section leader in Standford Code in Place program","type":"post"},{"authors":["Bingyuan Liu","Jiantao Zhang","Xiaoting Zhang","Wei Zhang","Chuanhui Yu","Yuan Zhou"],"categories":[],"content":"","date":1574294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574294400,"objectID":"75e3903746f33d2fda44c47d92a8db65","permalink":"https://by-liu.github.io/publication/furnishing-your-room-by-what-your-see/","publishdate":"2019-11-21T00:00:00Z","relpermalink":"/publication/furnishing-your-room-by-what-your-see/","section":"publication","summary":"","tags":[],"title":"Furnishing Your Room by What You See: An End-to-End Furniture Set Retrieval Framework with Rich Annotated Benchmark Dataset","type":"publication"},{"authors":["Bingyuan Liu","Jing Liu","Hanqing Lu"],"categories":[],"content":"","date":1438387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1438387200,"objectID":"6dd317cdaacf2e5a7704fd79539d7708","permalink":"https://by-liu.github.io/publication/detection-guided-deconvolutional-network/","publishdate":"2015-08-01T00:00:00Z","relpermalink":"/publication/detection-guided-deconvolutional-network/","section":"publication","summary":"Deep learning models have gained significant interest as a way of building hierarchical image representation. However, current models still perform far behind human vision system because of the lack of selective property, the lack of high-level guidance for learning and the weakness to learn from few examples. To address these problems, we propose a detection-guided hierarchical learning algorithm for image representation. First, we train a multi-layer deconvolutional network in an unsupervised bottom-up scheme. During the training process, we use each raw image as an input, and decompose an image using multiple alternating layers of non-negative convolutional sparse coding and max-pooling. Inspired from the observation that the filters in top layer can be selectively activated by different high-level structures of images, i.e., one or partial filters should correspond to a particular object class, we update the filters in network by minimizing the reconstruction errors of the corresponding feature maps with respect to certain object detection maps obtained by a set of pre-trained detectors. With the fine-tuned network, we can extract the features of given images in a purely unsupervised way with no need of detectors. We evaluate the proposed feature representation on the task of object recognition, for which an SVM classifier with spatial pyramid matching kernel is used. Experiments on the datasets of PASCAL VOC 2007, Caltech-101 and Caltech-256 demonstrate that our approach outperforms some recent hierarchical feature descriptors as well as classical hand-crafted features.","tags":["deep learning","image classification","object detection"],"title":"Detection guided deconvolutional network for hierarchical feature learning","type":"publication"},{"authors":["Bingyuan Liu","Jing Liu","Hanqing Lu"],"categories":[],"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"c13b3842e69645e6c7e5f69210096ed5","permalink":"https://by-liu.github.io/publication/learning-image-representation/","publishdate":"2015-07-01T00:00:00Z","relpermalink":"/publication/learning-image-representation/","section":"publication","summary":"How to build a suitable image representation remains a critical problem in computer vision. Traditional Bag-of-Feature (BoF) based models build image representation by the pipeline of local feature extraction, feature coding and spatial pooling. However, three major shortcomings hinder the performance, i.e., the limitation of hand-designed features, the discrimination loss in local appearance coding and the lack of spatial information. To overcome the above limitations, in this paper, we propose a generalized BoF-based framework, which is hierarchically learned by exploring recently developed deep learning methods. First, with raw images as input, we densely extract local patches and learn local features by stacked Independent Subspace Analysis network. The learned features are then transformed to appearance codes by sparse Restricted Boltzmann Machines. Second, we perform spatial max-pooling on a set of over-complete spatial regions, which is generated by covering various spatial distributions, to incorporate more flexible spatial information. Third, a structured sparse Auto-encoder is proposed to explore the region representations into the image-level signature. To learn the proposed hierarchy, we layerwise pre-train the network in unsupervised manner, followed by supervised fine-tuning with image labels. Extensive experiments on different benchmarks, i.e., UIUC-Sports, Caltech-101, Caltech-256, Scene-15 and MIT Indoor-67, demonstrate the effectiveness of our proposed model.","tags":["deep learning","unsupervised learning","image classification"],"title":"Learning representative and discriminative image representation by deep appearance and spatial coding","type":"publication"},{"authors":["Bingyuan Liu","Jing Liu","Zechao Li","Hanqing Lu"],"categories":[],"content":"","date":1414800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1414800000,"objectID":"e8a74cfc52d9102e13aac0459564a020","permalink":"https://by-liu.github.io/publication/image-representation-learning/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/image-representation-learning/","section":"publication","summary":"The bag of feature model is one of the most successful model to represent an image for classification task. However, the discrimination loss in the local appearance coding and the lack of spatial information hinder its performance. To address these problems, we propose a deep appearance and spatial coding model to build more optimal image representation for the classification task. The proposed model is a hierarchical architecture consisting of three operations: appearance coding, max-pooling and spatial coding. Firstly, with an image as input, we extract a set of local descriptors and adopt the appearance coding to encode them into high-dimensional robust vectors. Then max-pooling is performed within the over spatial partitioned grids to incorporate spatial information. After that, spatial coding is carried out to increasingly integrate the region vectors to a global image signature. Finally, the resulting image representation are employed to train a one-versus-others SVM classifier. In the learning of the proposed model, we layerwisely pre-train the network and then perform supervised fine-tuning with image labels. The experiments on three image benchmark datasets (i.e. 15-Scenes, PASCAL VOC 2007 and Caltech-256) demonstrate the effectiveness of our proposed model.","tags":["deep learning","sparse coding","image classification"],"title":"Image Representation Learning by Deep Appearance and Spatial Coding","type":"publication"},{"authors":["Bingyuan Liu","Jing Liu","Jingqiao Wang","Hanqing Lu"],"categories":[],"content":"","date":1414800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1414800000,"objectID":"e8510e5de2440d27549f68b25e68e6ed","permalink":"https://by-liu.github.io/publication/learning-a-representative-and-discriminative-part-model/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/learning-a-representative-and-discriminative-part-model/","section":"publication","summary":"The discovery of key and distinctive parts is critical for scene parsing and understanding. However, it is a challenging problem due to the weakly supervised condition, i.e., no annotation for parts is available. To address above issues, we propose a unified framework for learning a representative and discriminative part model with deep convolutional features. Firstly, we employ selective search method to generate regions that are more likely to be centered around the distinctive parts, which is used as parts training set. Then, the feature of each part region is extracted by forward propagating it into the Convolutional Neural Network (CNN). The CNN network is pre-trained by the large auxiliary ImageNet dataset and then fine-tuned on the particular scene images. To learn the parts model, we build a mid-level part dictionary based on sparse coding with a discriminative regularization. The two terms, i.e., the sparse reconstruction error term and the label consistent term, indicate the representative and discriminative properties respectively. Finally, we apply the learned parts model to build image-level representation for the scene recognition task. Extensive experiments demonstrate that we achieve state-of-the-art performances on the standard scene benchmarks, i.e. Scene-15 and MIT Indoor-67.","tags":["deep learning","sparse coding","scene recognition"],"title":"Learning a Representative and Discriminative Part Model with Deep Convolutional Features for Scene Recognition","type":"publication"},{"authors":["Bingyuan Liu","Jing Liu","Hanqing Lu"],"categories":[],"content":"","date":1413936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1413936000,"objectID":"71961fefc76019012fc5ef35e8dca64b","permalink":"https://by-liu.github.io/publication/adaptive-spatial-partition-learning/","publishdate":"2014-10-22T00:00:00Z","relpermalink":"/publication/adaptive-spatial-partition-learning/","section":"publication","summary":"Spatial Pyramid Matching is a successful extension of bag-of-feature model to embed spatial information of local features, in which the image is divided into a sequence of increasingly finer girds, and the grids are taken as uniform spatial partitions in ad-hoc manner without any theoretical motivation. Obviously, the uniform spatial partition cannot adapt to different spatial distribution across image categories. To this end, we propose a data-driven approach to adaptively learn the discriminative spatial partitions corresponding to each class, and explore them for image classification. First, a set of over-complete spatial partitions covering kinds of spatial distribution of local features are created in a flexible manner, and we concatenate the feature representations of each partitioned region. Then we adopt a discriminative learning formulation with the group sparse constraint to find a sparse mapping from the feature representation to the label space. To further enhance the robustness of the model, we compress the feature representation by removing the dimensions corresponding to those unimportant partitioned regions, and explore the compressed representation to generate a multi-region matching kernel prepared to train a one-versus-others SVM classifier. The experiments on three object datasets (i.e. Caltech-101, Caltech-256, Pascal VOC 2007), and one scene dataset (i.e. 15-Scenes) demonstrate the effectiveness of our proposed method.","tags":["sparse coding","image classification"],"title":"Adaptive spatial partition learning for image classification","type":"publication"},{"authors":["Bingyuan Liu","Jing Liu","Xiao Bai","Hanqing Lu"],"categories":[],"content":"","date":1408838400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1408838400,"objectID":"7e2013ed7493ee7c7719b2ca4f2b5d3b","permalink":"https://by-liu.github.io/publication/regularized-feature-learning/","publishdate":"2014-08-24T00:00:00Z","relpermalink":"/publication/regularized-feature-learning/","section":"publication","summary":"Recently, many deep networks are proposed to learn hierarchical image representation to replace traditional hand-designed features. To enhance the ability of the generative model to tackle discriminative computer vision tasks (e.g. image classification), we propose a hierarchical deconvolutional network with two biologically inspired properties incorporated, i.e., non-negative sparsity and selectivity. First, we propose a single layer deconvolutional model with a raw image as input, attempting to decompose the input as a weighted sum of feature maps convolving with filters. Here, the filters are the model parameters common to all the inputs, while the feature maps and the summing weights are specific to the input. The non-negative sparsity is formulated as the /i-norm regularizer on the feature map, which is used to generate feature representations for image classification. And the selectivity is forced on the filters to make different filters active different inputs, through requiring the sparsity on the summing weights specifically. The two properties are summarized into an overall cost function, which can be solved with an alternatively iterative algorithm. Then, we build multiple layer deconvolutional network by stacking the single models, where the next-layer inputs are the results of a 3D max-pooling operation on the inferred feature maps of the front layer, and train the network in a greedy layer wise scheme. Finally, we explore the feature maps of each layer to generate the image representations and input them to a SVM classifier for the classification task. Experiments on two image benchmark datasets of Caltech-101 and Caltech-256 demonstrate the encouraging performance of our model compared with other deep feature learning models as well as some hand-designed features.","tags":["deep learning","unsupervised learning","image classification"],"title":"Regularized Hierarchical Feature Learning with Non-negative Sparsity and Selectivity for Image Classification","type":"publication"},{"authors":["Bingyuan Liu","Jing Liu","Chunjie Zhang","Maolin Chen","Hanqing Lu"],"categories":[],"content":"","date":1374796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1374796800,"objectID":"6b9c5df9b7b04bfe5e2bb69fba404275","permalink":"https://by-liu.github.io/publication/robust-feature-encoding/","publishdate":"2013-07-26T00:00:00Z","relpermalink":"/publication/robust-feature-encoding/","section":"publication","summary":"The bag of visual words (BoW) model is one of the most successful model in image classification task. However, the major problem of the BoW model lies in the determination of visual words, which consists of codebook training and feature encoding phases. The traditional K-means and hard-assignment method completely ignore the structure of the local feature space, leading to high loss of information. To alleviate the information loss, we propose to incorporate the neighborhood information of the features into the codebook training and feature encoding process. We firstly propose a model to roughly measure the influence of the distribution of the neighboring features. Then we combine the proposed model with the traditional K-means method in a probability perspective to train the visual codebook. Finally, in the feature encoding phase, both the hard-assignment and soft-assignment method are improved with the proposed neighborhood information term. We investigate our method on two popular datasets: 15-Scenes and Caltech-101. Experimental results demonstrate the effectiveness of our proposed method.","tags":["image classification"],"title":"Robust Feature Encoding with Neighborhood Information for Image Classification","type":"publication"}]