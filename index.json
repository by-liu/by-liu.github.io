[{"authors":null,"categories":null,"content":"I am a Research Associate at ETS Montreal, Canada. I work at the Laboratoire d’Imagerie, de Vision et d’Intelligence Artificielle (LIVIA), under the supervision of Prof. Ismail Ben Ayed and Prof. Jose Dolz. My current research field is Computer Vision and its application in Medical Image Analysis.\nPreviously, I obtained advanced industrial experiences (R\u0026amp;D for real products), as I worked for several companies (Coohom, Alibaba, etc.) from 2015 to 2020. I got my Ph.D at Institue of Automatoin, Chinese Academy of Sciences in 2015, where my supervisors were Prof. Hanqing Lu and Prof. Jing Liu. I received my BSc from Zhejiang University in 2010.\nBesides tech stuff, I have passion in sports, reading and travel.\n","date":1372636800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1372636800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Research Associate at ETS Montreal, Canada. I work at the Laboratoire d’Imagerie, de Vision et d’Intelligence Artificielle (LIVIA), under the supervision of Prof. Ismail Ben Ayed and Prof.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://by-liu.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":[],"categories":[],"content":"To check the paper, please refer to: Segmentation with mixed supervision: Confidence maximization helps knowledge distillation.\n","date":1669872427,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669872427,"objectID":"fee8445d63f0a057ae917ad65e970f6d","permalink":"https://by-liu.github.io/post/media2022-confkd/","publishdate":"2022-12-01T00:27:07-05:00","relpermalink":"/post/media2022-confkd/","section":"post","summary":"To check the paper, please refer to: Segmentation with mixed supervision: Confidence maximization helps knowledge distillation.","tags":[],"title":"One paper regarding segmentation with mixed supervision is accpeted at MedIA","type":"post"},{"authors":["Bingyuan Liu","Jérôme Rony","Adrian Galdran","Jose Dolz","Ismail Ben Ayed"],"categories":[],"content":"","date":1669593600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669593600,"objectID":"5a87978955bee92d64f0ba774b80b1c2","permalink":"https://by-liu.github.io/publication/class-adaptive-network-calibration/","publishdate":"2022-11-30T00:00:00Z","relpermalink":"/publication/class-adaptive-network-calibration/","section":"publication","summary":"Recent studies have revealed that, beyond conventional accuracy, calibration should also be considered for training modern deep neural networks. To address miscalibration during learning, some methods have explored different penalty functions as part of the learning objective, alongside a standard classification loss, with a hyper-parameter controlling the relative contribution of each term. Nevertheless, these methods share two major drawbacks: 1) the scalar balancing weight is the same for all classes, hindering the ability to address different intrinsic difficulties or imbalance among classes; and 2) the balancing weight is usually fixed without an adaptive strategy, which may prevent from reaching the best compromise between accuracy and calibration, and requires hyper-parameter search for each application. We propose Class Adaptive Label Smoothing (CALS) for calibrating deep networks, which allows to learn class-wise multipliers during training, yielding a powerful alternative to common label smoothing penalties. Our method builds on a general Augmented Lagrangian approach, a well-established technique in constrained optimization, but we introduce several modifications to tailor it for large-scale, class-adaptive training. Comprehensive evaluation and multiple comparisons on a variety of benchmarks, including standard and long-tailed image classification, semantic segmentation, and text classification, demonstrate the superiority of the proposed method. The code is available at https://github.com/by-liu/CALS.","tags":["calibration","image classification","deep learning"],"title":"Class Adaptive Network Calibration","type":"publication"},{"authors":["Bingyuan Liu","Christian Desrosiers","Ismail Ben Ayed","Jose Dolz"],"categories":[],"content":"","date":1667779200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667779200,"objectID":"cffb7ed4ce3e680d53fe26818519631f","permalink":"https://by-liu.github.io/publication/mixed-supervised-segmentation-confidence-maximization/","publishdate":"2021-10-07T00:00:00Z","relpermalink":"/publication/mixed-supervised-segmentation-confidence-maximization/","section":"publication","summary":"Despite achieving promising results in a breadth of medical image segmentation tasks, deep neural networks require large training datasets with pixel-wise annotations. Obtaining these curated datasets is a cumbersome process which limits the applicability in scenarios. Mixed supervision is an appealing alternative for mitigating this obstacle. In this work, we propose a dual-branch architecture, where the upper branch (teacher) receives strong annotations, while the bottom one (student) is driven by limited supervision and guided by the upper branch. Combined with a standard cross-entropy loss over the labeled pixels, our novel formulation integrates two important terms: (i) a Shannon entropy loss defined over the less-supervised images, which encourages confident student predictions in the bottom branch; and (ii) a KL divergence term, which transfers the knowledge (i.e., predictions) of the strongly supervised branch to the less-supervised branch and guides the entropy (student-confidence) term to avoid trivial solutions. We show that the synergy between the entropy and KL divergence yields substantial improvements in performance. We also discuss an interesting link between Shannon-entropy minimization and standard pseudo-mask generation, and argue that the former should be preferred over the latter for leveraging information from unlabeled pixels. We evaluate the effectiveness of the proposed formulation through a series of quantitative and qualitative experiments using two publicly available datasets. Results demonstrate that our method significantly outperforms other strategies for semantic segmentation within a mixed-supervision framework, as well as recent semi-supervised approaches. Our code is publicly available: https://github.com/by-liu/ConfKD.","tags":["image segmentation","medical image segmentation","knowledge distillation"],"title":"Segmentation with mixed supervision: Confidence maximization helps knowledge distillation","type":"publication"},{"authors":["Bingyuan Liu","Ismail Ben Ayed","Adrian Galdran","Jose Dolz"],"categories":[],"content":"","date":1655769600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655769600,"objectID":"c6fb257c375e10e52e38929aea7594a5","permalink":"https://by-liu.github.io/publication/margin-based-label-smoothing/","publishdate":"2022-01-18T00:00:00Z","relpermalink":"/publication/margin-based-label-smoothing/","section":"publication","summary":"In spite of the dominant performances of deep neural networks, recent works have shown that they are poorly calibrated, resulting in over-confident predictions. Miscalibration can be exacerbated by overfitting due to the minimization of the cross-entropy during training, as it promotes the predicted softmax probabilities to match the one-hot label assignments. This yields a pre-softmax activation of the correct class that is significantly larger than the remaining activations. Recent evidence from the literature suggests that loss functions that embed implicit or explicit maximization of the entropy of predictions yield state-of-the-art calibration performances. We provide a unifying constrained-optimization perspective of current state-of-the-art calibration losses. Specifically, these losses could be viewed as approximations of a linear penalty (or a Lagrangian) imposing equality constraints on logit distances. This points to an important limitation of such underlying equality constraints, whose ensuing gradients constantly push towards a non-informative solution, which might prevent from reaching the best compromise between the discriminative performance and calibration of the model during gradient-based optimization. Following our observations, we propose a simple and flexible generalization based on inequality constraints, which imposes a controllable margin on logit distances. Comprehensive experiments on a variety of image classification, semantic segmentation and NLP benchmarks demonstrate that our method sets novel state-of-the-art results on these tasks in terms of network calibration, without affecting the discriminative performance. The code is available at https://github.com/by-liu/MbLS.","tags":["calibration","image classification","deep learning"],"title":"The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration","type":"publication"},{"authors":[],"categories":[],"content":"For details of this paper, please refer to : The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration.\n","date":1646973934,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646973934,"objectID":"a8832682d6bc6552078cdeeacac3d656","permalink":"https://by-liu.github.io/post/cvpr2022-mbls/","publishdate":"2022-03-11T12:45:34+08:00","relpermalink":"/post/cvpr2022-mbls/","section":"post","summary":"For details of this paper, please refer to : The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration.","tags":["calibration"],"title":"Our work on calibrating deep neural networks is accepted at CVPR 2022","type":"post"},{"authors":["Junde Wu","Huihui Fang","Fei Li","Huazhu Fu","Fengbin Lin","Jiongcheng Li","Lexing Huang","Qinji Yu","Sifan Song","Xingxing Xu","Yanyu Xu","Wensai Wang","Lingxiao Wang","Shuai Lu","Huiqi Li","Shihua Huang","Zhichao Lu","Chubin Ou","Xifei Wei","Bingyuan Liu","Riadh Kobbi","Xiaoying Tang","Li Lin","Qiang Zhou","Qiang Hu","Hrvoje Bogunovic","José Ignacio Orlando","Xiulan Zhang","Yanwu Xu"],"categories":[],"content":"","date":1644796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644796800,"objectID":"7ad23654727a71ea0b2899a517dbffd3","permalink":"https://by-liu.github.io/publication/gamma-challenge-2021/","publishdate":"2022-03-11T00:00:00Z","relpermalink":"/publication/gamma-challenge-2021/","section":"publication","summary":"Color fundus photography and Optical Coherence Tomography (OCT) are the two most cost-effective tools for glaucoma screening. Both two modalities of images have prominent biomarkers to indicate glaucoma suspected. Clinically, it is often recommended to take both of the screenings for a more accurate and reliable diagnosis. However, although numerous algorithms are proposed based on fundus images or OCT volumes in computer-aided diagnosis, there are still few methods leveraging both of the modalities for the glaucoma assessment. Inspired by the success of Retinal Fundus Glaucoma Challenge (REFUGE) we held previously, we set up the Glaucoma grAding from Multi-Modality imAges (GAMMA) Challenge to encourage the development of fundus \u0026 OCT-based glaucoma grading. The primary task of the challenge is to grade glaucoma from both the 2D fundus images and 3D OCT scanning volumes. As part of GAMMA, we have publicly released a glaucoma annotated dataset with both 2D fundus color photography and 3D OCT volumes, which is the first multi-modality dataset for glaucoma grading. In addition, an evaluation framework is also established to evaluate the performance of the submitted methods. During the challenge, 1272 results were submitted, and finally, top-10 teams were selected to the final stage. We analysis their results and summarize their methods in the paper. Since all these teams submitted their source code in the challenge, a detailed ablation study is also conducted to verify the effectiveness of the particular modules proposed. We find many of the proposed techniques are practical for the clinical diagnosis of glaucoma. As the first in-depth study of fundus \u0026 OCT multi-modality glaucoma grading, we believe the GAMMA Challenge will be an essential starting point for future research.","tags":["Glaucoma grading","multi-modal learning","medical image segmentation"],"title":"GAMMA Challenge:Glaucoma grAding from Multi-Modality imAges","type":"publication"},{"authors":[],"categories":[],"content":"Our team, DIAGNOS-ETS, achieved great performance on MICCAI2021 GAMMA contest. Here are our results on the three tasks:\n Top 1 : Macular fovea localization Top 1 : Optic disc/cup segmentation Top 8 : Multi-modal glaucoma grading  The total number of teams in this contest : 556  .\nThe collaborative paper of the top 10 teams in the challenge : GAMMA Challenge: Glaucoma grAding from Multi-Modality imAges\n The certificate for macular fovea localization task   The certificate for optic disc/cup segmentatoin   The certificate of excellent award  ","date":1634980684,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634980684,"objectID":"be787f7c5d00b8dc04459856d7f18908","permalink":"https://by-liu.github.io/post/gamma-2021/","publishdate":"2021-10-23T17:18:04+08:00","relpermalink":"/post/gamma-2021/","section":"post","summary":"Our team, DIAGNOS-ETS, achieved great performance on MICCAI2021 GAMMA contest. Here are our results on the three tasks:\n Top 1 : Macular fovea localization Top 1 : Optic disc/cup segmentation Top 8 : Multi-modal glaucoma grading  The total number of teams in this contest : 556  .","tags":["multi-modal learning","glaucoma grading","macular fovea localization","optic disc and cup segmentation","medical image segmentation"],"title":"Our results on MICCAI2021 GAMMA contest : Top 1 in two tasks ","type":"post"},{"authors":["Bingyuan Liu","Jose Dolz","Adrian Galdran","Riadh Kobbi","Ismail Ben Ayed"],"categories":[],"content":"","date":1618704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618704000,"objectID":"20b5ebf02c607fe149f67fe71809eb64","permalink":"https://by-liu.github.io/publication/the-hidden-biases-of-segmentation-losses/","publishdate":"2021-04-20T00:00:00Z","relpermalink":"/publication/the-hidden-biases-of-segmentation-losses/","section":"publication","summary":"Most segmentation losses are arguably variants of the Cross-Entropy (CE) or Dice losses. In the abundant segmentation literature, there is no clear consensus as to which of these losses is a better choice, with varying performances for each across different benchmarks and applications. In this work, we develop a theoretical analysis that links these two types of losses, exposing their advantages and weaknesses. First, we provide a constrained-optimization perspective showing that CE and Dice share a much deeper connection than previously thought: They both decompose into label-marginal penalties and closely related ground-truth matching penalties. Then, we provide bound relationships and an information-theoretic analysis, which uncover hidden label-marginal biases: Dice has an intrinsic bias towards specific extremely imbalanced solutions, whereas CE implicitly encourages the ground-truth region proportions. Our theoretical results explain the wide experimental evidence in the medical-imaging literature, whereby Dice losses bring improvements for imbalanced segmentation. It also explains why CE dominates natural-image problems with diverse class proportions, in which case Dice might have difficulty adapting to different label-marginal distributions. Based on our theoretical analysis, we propose a principled and simple solution, which enables to control explicitly the label-marginal bias. Our loss integrates CE with explicit 1 regularization, which encourages label marginals to match target class proportions, thereby mitigating class imbalance but without losing generality. Comprehensive experiments and ablation studies over different losses and applications validate our theoretical analysis, as well as the effectiveness of our explicit label-marginal regularizers.","tags":["image segmentation","medical image segmentation"],"title":"The hidden label-marginal biases of segmentation losses","type":"publication"},{"authors":["Bingyuan Liu","Jiantao Zhang","Xiaoting Zhang","Wei Zhang","Chuanhui Yu","Yuan Zhou"],"categories":[],"content":"","date":1574294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574294400,"objectID":"75e3903746f33d2fda44c47d92a8db65","permalink":"https://by-liu.github.io/publication/furnishing-your-room-by-what-your-see/","publishdate":"2019-11-21T00:00:00Z","relpermalink":"/publication/furnishing-your-room-by-what-your-see/","section":"publication","summary":"","tags":["constrastive learning","image retrieval"],"title":"Furnishing Your Room by What You See: An End-to-End Furniture Set Retrieval Framework with Rich Annotated Benchmark Dataset","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://by-liu.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://by-liu.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":null,"title":"Example Project","type":"project"},{"authors":["Bingyuan Liu","Jing Liu","Hanqing Lu"],"categories":[],"content":"","date":1438387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1438387200,"objectID":"6dd317cdaacf2e5a7704fd79539d7708","permalink":"https://by-liu.github.io/publication/detection-guided-deconvolutional-network/","publishdate":"2015-08-01T00:00:00Z","relpermalink":"/publication/detection-guided-deconvolutional-network/","section":"publication","summary":"Deep learning models have gained significant interest as a way of building hierarchical image representation. However, current models still perform far behind human vision system because of the lack of selective property, the lack of high-level guidance for learning and the weakness to learn from few examples. To address these problems, we propose a detection-guided hierarchical learning algorithm for image representation. First, we train a multi-layer deconvolutional network in an unsupervised bottom-up scheme. During the training process, we use each raw image as an input, and decompose an image using multiple alternating layers of non-negative convolutional sparse coding and max-pooling. Inspired from the observation that the filters in top layer can be selectively activated by different high-level structures of images, i.e., one or partial filters should correspond to a particular object class, we update the filters in network by minimizing the reconstruction errors of the corresponding feature maps with respect to certain object detection maps obtained by a set of pre-trained detectors. With the fine-tuned network, we can extract the features of given images in a purely unsupervised way with no need of detectors. We evaluate the proposed feature representation on the task of object recognition, for which an SVM classifier with spatial pyramid matching kernel is used. Experiments on the datasets of PASCAL VOC 2007, Caltech-101 and Caltech-256 demonstrate that our approach outperforms some recent hierarchical feature descriptors as well as classical hand-crafted features.","tags":["deep learning","image classification","object detection"],"title":"Detection guided deconvolutional network for hierarchical feature learning","type":"publication"},{"authors":["Bingyuan Liu","Jing Liu","Hanqing Lu"],"categories":[],"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"c13b3842e69645e6c7e5f69210096ed5","permalink":"https://by-liu.github.io/publication/learning-image-representation/","publishdate":"2015-07-01T00:00:00Z","relpermalink":"/publication/learning-image-representation/","section":"publication","summary":"How to build a suitable image representation remains a critical problem in computer vision. Traditional Bag-of-Feature (BoF) based models build image representation by the pipeline of local feature extraction, feature coding and spatial pooling. However, three major shortcomings hinder the performance, i.e., the limitation of hand-designed features, the discrimination loss in local appearance coding and the lack of spatial information. To overcome the above limitations, in this paper, we propose a generalized BoF-based framework, which is hierarchically learned by exploring recently developed deep learning methods. First, with raw images as input, we densely extract local patches and learn local features by stacked Independent Subspace Analysis network. The learned features are then transformed to appearance codes by sparse Restricted Boltzmann Machines. Second, we perform spatial max-pooling on a set of over-complete spatial regions, which is generated by covering various spatial distributions, to incorporate more flexible spatial information. Third, a structured sparse Auto-encoder is proposed to explore the region representations into the image-level signature. To learn the proposed hierarchy, we layerwise pre-train the network in unsupervised manner, followed by supervised fine-tuning with image labels. Extensive experiments on different benchmarks, i.e., UIUC-Sports, Caltech-101, Caltech-256, Scene-15 and MIT Indoor-67, demonstrate the effectiveness of our proposed model.","tags":["deep learning","unsupervised learning","image classification"],"title":"Learning representative and discriminative image representation by deep appearance and spatial coding","type":"publication"},{"authors":["Bingyuan Liu","Jing Liu","Zechao Li","Hanqing Lu"],"categories":[],"content":"","date":1414800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1414800000,"objectID":"e8a74cfc52d9102e13aac0459564a020","permalink":"https://by-liu.github.io/publication/image-representation-learning/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/image-representation-learning/","section":"publication","summary":"The bag of feature model is one of the most successful model to represent an image for classification task. However, the discrimination loss in the local appearance coding and the lack of spatial information hinder its performance. To address these problems, we propose a deep appearance and spatial coding model to build more optimal image representation for the classification task. The proposed model is a hierarchical architecture consisting of three operations: appearance coding, max-pooling and spatial coding. Firstly, with an image as input, we extract a set of local descriptors and adopt the appearance coding to encode them into high-dimensional robust vectors. Then max-pooling is performed within the over spatial partitioned grids to incorporate spatial information. After that, spatial coding is carried out to increasingly integrate the region vectors to a global image signature. Finally, the resulting image representation are employed to train a one-versus-others SVM classifier. In the learning of the proposed model, we layerwisely pre-train the network and then perform supervised fine-tuning with image labels. The experiments on three image benchmark datasets (i.e. 15-Scenes, PASCAL VOC 2007 and Caltech-256) demonstrate the effectiveness of our proposed model.","tags":["deep learning","sparse coding","image classification"],"title":"Image Representation Learning by Deep Appearance and Spatial Coding","type":"publication"},{"authors":["Bingyuan Liu","Jing Liu","Jingqiao Wang","Hanqing Lu"],"categories":[],"content":"","date":1414800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1414800000,"objectID":"e8510e5de2440d27549f68b25e68e6ed","permalink":"https://by-liu.github.io/publication/learning-a-representative-and-discriminative-part-model/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/learning-a-representative-and-discriminative-part-model/","section":"publication","summary":"The discovery of key and distinctive parts is critical for scene parsing and understanding. However, it is a challenging problem due to the weakly supervised condition, i.e., no annotation for parts is available. To address above issues, we propose a unified framework for learning a representative and discriminative part model with deep convolutional features. Firstly, we employ selective search method to generate regions that are more likely to be centered around the distinctive parts, which is used as parts training set. Then, the feature of each part region is extracted by forward propagating it into the Convolutional Neural Network (CNN). The CNN network is pre-trained by the large auxiliary ImageNet dataset and then fine-tuned on the particular scene images. To learn the parts model, we build a mid-level part dictionary based on sparse coding with a discriminative regularization. The two terms, i.e., the sparse reconstruction error term and the label consistent term, indicate the representative and discriminative properties respectively. Finally, we apply the learned parts model to build image-level representation for the scene recognition task. Extensive experiments demonstrate that we achieve state-of-the-art performances on the standard scene benchmarks, i.e. Scene-15 and MIT Indoor-67.","tags":["deep learning","sparse coding","scene recognition"],"title":"Learning a Representative and Discriminative Part Model with Deep Convolutional Features for Scene Recognition","type":"publication"},{"authors":["Bingyuan Liu","Jing Liu","Hanqing Lu"],"categories":[],"content":"","date":1413936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1413936000,"objectID":"71961fefc76019012fc5ef35e8dca64b","permalink":"https://by-liu.github.io/publication/adaptive-spatial-partition-learning/","publishdate":"2014-10-22T00:00:00Z","relpermalink":"/publication/adaptive-spatial-partition-learning/","section":"publication","summary":"Spatial Pyramid Matching is a successful extension of bag-of-feature model to embed spatial information of local features, in which the image is divided into a sequence of increasingly finer girds, and the grids are taken as uniform spatial partitions in ad-hoc manner without any theoretical motivation. Obviously, the uniform spatial partition cannot adapt to different spatial distribution across image categories. To this end, we propose a data-driven approach to adaptively learn the discriminative spatial partitions corresponding to each class, and explore them for image classification. First, a set of over-complete spatial partitions covering kinds of spatial distribution of local features are created in a flexible manner, and we concatenate the feature representations of each partitioned region. Then we adopt a discriminative learning formulation with the group sparse constraint to find a sparse mapping from the feature representation to the label space. To further enhance the robustness of the model, we compress the feature representation by removing the dimensions corresponding to those unimportant partitioned regions, and explore the compressed representation to generate a multi-region matching kernel prepared to train a one-versus-others SVM classifier. The experiments on three object datasets (i.e. Caltech-101, Caltech-256, Pascal VOC 2007), and one scene dataset (i.e. 15-Scenes) demonstrate the effectiveness of our proposed method.","tags":["sparse coding","image classification"],"title":"Adaptive spatial partition learning for image classification","type":"publication"},{"authors":["Bingyuan Liu","Jing Liu","Xiao Bai","Hanqing Lu"],"categories":[],"content":"","date":1408838400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1408838400,"objectID":"7e2013ed7493ee7c7719b2ca4f2b5d3b","permalink":"https://by-liu.github.io/publication/regularized-feature-learning/","publishdate":"2014-08-24T00:00:00Z","relpermalink":"/publication/regularized-feature-learning/","section":"publication","summary":"Recently, many deep networks are proposed to learn hierarchical image representation to replace traditional hand-designed features. To enhance the ability of the generative model to tackle discriminative computer vision tasks (e.g. image classification), we propose a hierarchical deconvolutional network with two biologically inspired properties incorporated, i.e., non-negative sparsity and selectivity. First, we propose a single layer deconvolutional model with a raw image as input, attempting to decompose the input as a weighted sum of feature maps convolving with filters. Here, the filters are the model parameters common to all the inputs, while the feature maps and the summing weights are specific to the input. The non-negative sparsity is formulated as the /i-norm regularizer on the feature map, which is used to generate feature representations for image classification. And the selectivity is forced on the filters to make different filters active different inputs, through requiring the sparsity on the summing weights specifically. The two properties are summarized into an overall cost function, which can be solved with an alternatively iterative algorithm. Then, we build multiple layer deconvolutional network by stacking the single models, where the next-layer inputs are the results of a 3D max-pooling operation on the inferred feature maps of the front layer, and train the network in a greedy layer wise scheme. Finally, we explore the feature maps of each layer to generate the image representations and input them to a SVM classifier for the classification task. Experiments on two image benchmark datasets of Caltech-101 and Caltech-256 demonstrate the encouraging performance of our model compared with other deep feature learning models as well as some hand-designed features.","tags":["deep learning","unsupervised learning","image classification"],"title":"Regularized Hierarchical Feature Learning with Non-negative Sparsity and Selectivity for Image Classification","type":"publication"},{"authors":["Bingyuan Liu","Jing Liu","Chunjie Zhang","Maolin Chen","Hanqing Lu"],"categories":[],"content":"","date":1374796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1374796800,"objectID":"6b9c5df9b7b04bfe5e2bb69fba404275","permalink":"https://by-liu.github.io/publication/robust-feature-encoding/","publishdate":"2013-07-26T00:00:00Z","relpermalink":"/publication/robust-feature-encoding/","section":"publication","summary":"The bag of visual words (BoW) model is one of the most successful model in image classification task. However, the major problem of the BoW model lies in the determination of visual words, which consists of codebook training and feature encoding phases. The traditional K-means and hard-assignment method completely ignore the structure of the local feature space, leading to high loss of information. To alleviate the information loss, we propose to incorporate the neighborhood information of the features into the codebook training and feature encoding process. We firstly propose a model to roughly measure the influence of the distribution of the neighboring features. Then we combine the proposed model with the traditional K-means method in a probability perspective to train the visual codebook. Finally, in the feature encoding phase, both the hard-assignment and soft-assignment method are improved with the proposed neighborhood information term. We investigate our method on two popular datasets: 15-Scenes and Caltech-101. Experimental results demonstrate the effectiveness of our proposed method.","tags":["image classification"],"title":"Robust Feature Encoding with Neighborhood Information for Image Classification","type":"publication"},{"authors":["","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"4870f26c8ceab84e13d7755aa5d855f2","permalink":"https://by-liu.github.io/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/example/","section":"","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://by-liu.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"Here is the places I have lived or visited so far :\n There are so many wonderful places I have not traveled yet. The next one may be somewhere in Europe.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5c2f58f90a624906b9d8c0eb0f85d9c6","permalink":"https://by-liu.github.io/travel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/travel/","section":"","summary":"Here is the places I have lived or visited so far :\n There are so many wonderful places I have not traveled yet. The next one may be somewhere in Europe.","tags":null,"title":"My travel map","type":"page"}]